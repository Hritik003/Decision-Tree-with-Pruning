# -*- coding: utf-8 -*-
"""decision Tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ln_mTJDCXCWaTXtsLlaepwqtbVWUFKP6

#**Step0: Loading the Data and Libraries**
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random as rd
data=pd.read_csv('/content/gdrive/MyDrive/ML ASSIGNMENT-1/adult.csv')
test=pd.read_csv('/content/gdrive/MyDrive/ML ASSIGNMENT-1/census-income.test.csv')

"""#**Step1: Data & Test info**"""

data.head()

test.drop(columns=test.columns[0], axis=1, inplace=True)
test.head()

# Shape of dataset
print('Rows: {} Cols: {}'.format(data.shape[0], data.shape[1]))

# Shape of Testset
print('Rows: {} Cols: {}'.format(test.shape[0], test.shape[1]))

# Features data-type
data.info()

# Features Test-type
test.info()

#Data Statistics
data.describe().T

#Test Statistics
test.describe().T

# Check for null values
round((data.isnull().sum() / data.shape[0]) * 100, 2).astype(str) + ' %'

# Check for null values
round((test.isnull().sum() / test.shape[0]) * 100, 2).astype(str) + ' %'

"""# **Step2: Data Preprocessing**

##Dataset

###*Feature Engineering Task 1*
"""

data = data.replace('?', np.nan)

# Checking null values with '?'
round((data.isnull().sum() / data.shape[0]) * 100, 2).astype(str) + ' %'

nan_cols = ['workclass', 'occupation', 'native.country']

for col in nan_cols:
    data[col].fillna(data[col].mode()[0], inplace=True)

"""###*Feature Engineering Task 2*"""

data = data.replace(0, np.nan)

# Checking null values with 0
round((data.isnull().sum() / data.shape[0]) * 100, 2).astype(str) + ' %'

zero_nan_cols = ['capital.gain','capital.loss']

for col in zero_nan_cols:
    data[col].fillna(data[col].mean(), inplace=True)

round((data.isnull().sum() / data.shape[0]) * 100, 2).astype(str) + ' %'

data.describe().T

data = data.replace('<=50K', 0)
data = data.replace('>50K', 1)
data.head(30)

"""###*Equal-Width Discretization*"""

# Define the continuous attributes that need to be discretized
continuous_attributes = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']

# Discretize the continuous attributes using appropriate techniques
num_bins = 10  # Define the number of bins for discretization

for attribute in continuous_attributes:
    # Perform discretization using the cut() function
    data[attribute+'_discrete'] = pd.cut(data[attribute], num_bins)

# Print the updated Data with discretized attributes
print(data.head())

"""###Discretized Data"""

discrete_data = data.drop(['age'],axis=1)
discrete_data = discrete_data.drop(['fnlwgt'],axis=1)
discrete_data = discrete_data.drop(['education.num'],axis=1)
discrete_data = discrete_data.drop(['capital.gain'],axis=1)
discrete_data = discrete_data.drop(['capital.loss'],axis=1)
discrete_data = discrete_data.drop(['hours.per.week'],axis=1)
discrete_data.head()

"""###Label Encoding"""

from sklearn.preprocessing import LabelEncoder

labels=['age_discrete', 'workclass', 'fnlwgt_discrete', 'education', 'education.num_discrete',
       'marital.status', 'occupation', 'relationship', 'race', 'sex',
       'capital.gain_discrete', 'capital.loss_discrete', 'hours.per.week_discrete', 'native.country']
for label in labels:
  discrete_data[label+'_n']= LabelEncoder().fit_transform(discrete_data[label])
discrete_data.head()

discrete_data=discrete_data.drop(['age_discrete', 'workclass', 'fnlwgt_discrete', 'education', 'education.num_discrete',
       'marital.status', 'occupation', 'relationship', 'race', 'sex',
       'capital.gain_discrete', 'capital.loss_discrete', 'hours.per.week_discrete', 'native.country'], axis='columns')

discrete_data.head()

"""##Test set

###Feature Engineering Task 1
"""

for col in test.columns:
    if test[col].dtype == 'object':
        test[col] = test[col].str.strip()

test.replace("?", np.nan, inplace=True)
round((test.isnull().sum() / test.shape[0]) * 100, 2).astype(str) + ' %'

nan_cols = ['Workclass','Occupation','Native country']
# test.head()

for c in nan_cols:
    test[c].fillna(test[c].mode()[0], inplace=True)

test.head()

"""###Feature Engineering Task 2"""

test = test.replace(0, np.nan)

# Checking null values with '?'
round((test.isnull().sum() / test.shape[0]) * 100, 2).astype(str) + ' %'

zero_nan_cols = ['Capital gain','Capital loss']

for c in zero_nan_cols:
    test[c].fillna(test[c].mean(), inplace=True)

test.head()

test = test.replace('<=50K.', 0)
test = test.replace('>50K.', 1)
test.head(30)

"""###*Equal-Width Discretization*"""

# Define the continuous attributes that need to be discretized
continuous_attributes = ['Age', 'Fnlwgt', 'Educatio sum', 'Capital gain', 'Capital loss', 'Hr/week']

# Discretize the continuous attributes using appropriate techniques
num_bins = 10  # Define the number of bins for discretization

for attribute in continuous_attributes:
    # Perform discretization using the cut() function
    test[attribute+'_discrete'] = pd.cut(test[attribute], num_bins)

# Print the updated Data with discretized attributes
print(test.head())

"""###Discretized Data"""

discrete_test = test.drop(['Age'],axis=1)
discrete_test = discrete_test.drop(['Fnlwgt'],axis=1)
discrete_test = discrete_test.drop(['Educatio sum'],axis=1)
discrete_test = discrete_test.drop(['Capital gain'],axis=1)
discrete_test = discrete_test.drop(['Capital loss'],axis=1)
discrete_test = discrete_test.drop(['Hr/week'],axis=1)
discrete_test.shape[0]
discrete_test.head()

# Rename column 'A' to 'X'
discrete_test = discrete_test.rename(columns={'Occupation':'occupation','Maritial Status':'marital.status','Relationship':'relationship','Workclass':'workclass','Education':'education','Race':'race','Sex':'sex','Native country':'native.country','Class':'income','Hr/week_discrete':'hours.per.week_discrete','Age_discrete': 'age_discrete','Fnlwgt_discrete':'fnlwgt_discrete','Educatio sum_discrete':'education.num_discrete','Capital gain_discrete':'capital.gain_discrete','Capital loss_discrete':'capital.loss_discrete'})

"""###Label Encoding"""

from sklearn.preprocessing import LabelEncoder

labels=['age_discrete', 'workclass', 'fnlwgt_discrete', 'education', 'education.num_discrete',
       'marital.status', 'occupation', 'relationship', 'race', 'sex',
       'capital.gain_discrete', 'capital.loss_discrete', 'hours.per.week_discrete', 'native.country']
for label in labels:
  discrete_test[label+'_n']= LabelEncoder().fit_transform(discrete_test[label])
discrete_test.head()

discrete_test=discrete_test.drop(['age_discrete', 'workclass', 'fnlwgt_discrete', 'education', 'education.num_discrete',
       'marital.status', 'occupation', 'relationship', 'race', 'sex',
       'capital.gain_discrete', 'capital.loss_discrete', 'hours.per.week_discrete', 'native.country'], axis='columns')

discrete_test.head()

"""# Step 3.1 : Decision Tree Classifier

##Node class
"""

class Node():
    def __init__(self, feature_index=None, threshold=None, children=None, node_dataset =None,IG=None, value=None):
        ''' constructor '''

        # for decision node
        self.feature_index = feature_index
        self.threshold = threshold
        self.children=children
        self.node_dataset=node_dataset
        self.IG = IG

        # for leaf node
        self.value=value

"""##Tree class"""

import random


class Node:
    def __init__(self, gini, num_samples, num_samples_per_class, predicted_class):
        self.gini = gini
        self.num_samples = num_samples
        self.num_samples_per_class = num_samples_per_class
        self.predicted_class = predicted_class
        self.feature_index = 0
        self.threshold = 0
        self.left = None
        self.right = None

class DecisionTree:
    def __init__(self, max_depth=None):
        self.root = None
        self.max_depth = max_depth

    def from_node(self, node, max_depth=None):
        self.tree_ = node
        self.max_depth = max_depth
        return self


    def _gini(self, y):
        m = y.size
        return 1.0 - sum((np.bincount(y) / m) ** 2)

    def _best_split(self, X, y):
        m = y.size
        if m <= 1:
            return None, None
        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]
        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)
        best_idx, best_thr = None, None
        for idx in range(self.n_features_):
            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))
            num_left = [0] * self.n_classes_
            num_right = num_parent.copy()
            for i in range(1, m):
                c = classes[i - 1]
                num_left[c] += 1
                num_right[c] -= 1
                gini_left = 1.0 - sum(
                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)
                )
                gini_right = 1.0 - sum(
                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)
                )
                gini = (i * gini_left + (m - i) * gini_right) / m
                if thresholds[i] == thresholds[i - 1]:
                    continue
                if gini < best_gini:
                    best_gini = gini
                    best_idx = idx
                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2
        return best_idx, best_thr

    def _grow_tree(self, X, y, depth=0):
        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]
        predicted_class = np.argmax(num_samples_per_class)
        node = Node(
            gini=self._gini(y),
            num_samples=y.size,
            num_samples_per_class=num_samples_per_class,
            predicted_class=predicted_class,
        )

        if depth < self.max_depth:
            idx, thr = self._best_split(X, y)
            if idx is not None:
                indices_left = X[:, idx] < thr
                X_left, y_left = X[indices_left], y[indices_left]
                X_right, y_right = X[~indices_left], y[~indices_left]
                node.feature_index = idx
                node.threshold = thr
                node.left = self._grow_tree(X_left, y_left, depth + 1)
                node.right = self._grow_tree(X_right, y_right, depth + 1)
        return node

    def fit(self, X, y):
        X, y = np.array(X), np.array(y)  # Conversion from DataFrame to NumPy array
        self.n_classes_ = len(set(y))
        self.n_features_ = X.shape[1]
        self.tree_ = self._grow_tree(X, y)

    def _predict(self, inputs, node):
        if node.left is None and node.right is None:
            return node.predicted_class
        else:
            if inputs[node.feature_index] < node.threshold:
                return self._predict(inputs, node.left)
            else:
                return self._predict(inputs, node.right)

    def predict(tree, X):
      if isinstance(tree, DecisionTree):
          X = np.array(X)  # Conversion from DataFrame to NumPy array
          return [tree._predict(inputs, tree.tree_) for inputs in X]
      else:
          print("The tree object passed is not an instance of the DecisionTree class.")



    def print_tree(self, node=None, depth=0):
        """
        Print the decision tree in a recursive manner.
        """
        if node is None:
            node = self.tree_  # If no node provided, print from root

        if node:
            print(depth * '  ', f"[{node.feature_index} < {node.threshold}] gini={node.gini} samples={node.num_samples} value={node.num_samples_per_class} class={node.predicted_class}")
            self.print_tree(node.left, depth + 1)
            self.print_tree(node.right, depth + 1)

    def score(self, X, y, sample_weight=None):
        from sklearn.metrics import accuracy_score

        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)




def reduced_error_prune(decision_tree, X, y):

  if decision_tree.left == None and decision_tree.right == None:
      return decision_tree
  elif decision_tree.left != None and decision_tree.right != None:
      decision_tree.left = reduced_error_prune(decision_tree.left, X, y)
      decision_tree.right = reduced_error_prune(decision_tree.right, X, y)
  else:
      before_pruning_score = accuracy(y, decision_tree.predict(X))
      if decision_tree.left != None:
          temp_node = decision_tree.left
          decision_tree.left = None
      else:
          temp_node = decision_tree.right
          decision_tree.right = None
      after_pruning_score = accuracy(y, decision_tree.predict(X))
      if before_pruning_score < after_pruning_score:
          return decision_tree
      else:
          if decision_tree.left == None:
              decision_tree.left = temp_node
          else:
              decision_tree.right = temp_node
          return decision_tree
  return decision_tree



def accuracy(y_true, y_pred):
    accuracy = np.sum(y_true == y_pred) / len(y_true)
    return accuracy

"""##Fit the Model

###Fitting without Library
"""

dt_y_train=discrete_data.income
dt_x_train=discrete_data.drop(['income'],axis=1);
dt_y_test=discrete_test.income
dt_x_test=discrete_test.drop(['income'],axis=1);

print(dt_x_test.shape)

"""###Test-Validation 50/50 split"""

dt_x_val = dt_x_test.iloc[0:int(0.5*dt_x_test.shape[0])+1]
dt_y_val = dt_y_test.iloc[0:int(0.5*dt_y_test.shape[0])+1]

dt_y_val.head()

"""#### Graph Before Pruning"""

n=dt_x_train.shape[1]
x_ax=[]
y_ax_test=[]
y_ax_train=[]

for i in range(n, 1, -1):
    classifier = DecisionTree(max_depth=i)
    classifier.fit(dt_x_train, dt_y_train)
    x_ax.append(i)
    y_ax_test.append(classifier.score(dt_x_test, dt_y_test))
    y_ax_train.append(classifier.score(dt_x_train, dt_y_train))

plt.plot(x_ax,y_ax_test, label ="testing accuracy")
plt.plot(x_ax,y_ax_train,label ="training accuracy")

# Add labels for the x-axis and y-axis
plt.xlabel("X-axis label")
plt.ylabel("Y-axis label")

# Add a title to the plot
plt.title("Accuracy Comparison")

# Add a legend to the plot
plt.legend()

# Display the plot
plt.show()

classifier.print_tree()

"""####*Training Accuracy before pruning*"""

Y_pred = classifier.predict(dt_x_train)
from sklearn.metrics import accuracy_score
round((accuracy_score(dt_y_train,Y_pred))*100,2).astype(str)+'%'

"""####*Testing Accuracy before pruning*"""

Y_pred = classifier.predict(dt_x_test)
from sklearn.metrics import accuracy_score
round((accuracy_score(dt_y_test,Y_pred))*100,2).astype(str)+'%'

"""####*Validation Accuracy before Pruning*"""

Y_pred = classifier.predict(dt_x_val)
from sklearn.metrics import accuracy_score
round((accuracy_score(dt_y_val,Y_pred))*100,2).astype(str)+'%'

"""###post-Pruning"""

n=dt_x_train.shape[1]
x_ax=[]
y_ax_test=[]
y_ax_val=[]

for i in range(n, 1, -1):
    if i<=5: pruned_3 = DecisionTree(max_depth=i)
    else: pruned_3 = DecisionTree(max_depth=5)
    pruned_3.fit(dt_x_train, dt_y_train)
    pruned_tree = reduced_error_prune(pruned_3.tree_, dt_x_val, dt_y_val)
    pruned_3.tree_=pruned_tree
    x_ax.append(i)
    y_ax_test.append(pruned_3.score(dt_x_test, dt_y_test))
    y_ax_val.append(pruned_3.score(dt_x_val, dt_y_val))

"""####Graph after post-pruning"""

plt.plot(x_ax,y_ax_test, label ="testing accuracy")
plt.plot(x_ax,y_ax_val,label ="validation accuracy")

# Add labels for the x-axis and y-axis
plt.xlabel("Depth of the nodes")
plt.ylabel("Accuracy")

# Add a title to the plot
plt.title("Accuracy Comparison")

# Add a legend to the plot
plt.legend()

# Display the plot
plt.show()

pruned_3=DecisionTree(max_depth=14)
pruned_3.fit(dt_x_train, dt_y_train)
pruned_tree = reduced_error_prune(pruned_3.tree_, dt_x_train, dt_y_train)
# pruned_3.tree_=pruned_tree

"""####*Training Accuracy after pruning*"""

# Make predictions
pruned_tree_obj = DecisionTree()
pruned_tree_obj.from_node(pruned_tree)
y_pred_prune = pruned_tree_obj.predict(dt_x_train)

from sklearn.metrics import accuracy_score
round((accuracy_score(dt_y_train,y_pred_prune))*100,2).astype(str)+'%'

"""####*Testing Accuracy after pruning*"""

# Make predictions
pruned_tree_obj = DecisionTree()
pruned_tree_obj.from_node(pruned_tree)
y_pred_prune = pruned_tree_obj.predict(dt_x_test)

from sklearn.metrics import accuracy_score
round((accuracy_score(dt_y_test,y_pred_prune))*100,2).astype(str)+'%'

"""####*Validation Accuracy after pruning*"""

# Make predictions
pruned_tree_obj = DecisionTree()
pruned_tree_obj.from_node(pruned_tree)
y_pred_prune = pruned_tree_obj.predict(dt_x_val)

from sklearn.metrics import accuracy_score
round((accuracy_score(dt_y_val,y_pred_prune))*100,2).astype(str)+'%'

"""###Fitting using Library

####Graph before pruning
"""

n=dt_x_train.shape[1]
dt_x_ax=[]
dt_y_ax_test=[]
dt_y_ax_train=[]

from sklearn.tree import DecisionTreeClassifier
for i in range(n, 1, -1):
    model = DecisionTreeClassifier(max_leaf_nodes=i)
    model.fit(dt_x_train, dt_y_train)
    dt_x_ax.append(i)
    dt_y_ax_test.append(model.score(dt_x_test, dt_y_test))
    dt_y_ax_train.append(model.score(dt_x_train, dt_y_train))

plt.plot(dt_x_ax,dt_y_ax_test, label ="testing accuracy")
plt.plot(dt_x_ax,dt_y_ax_train,label ="training accuracy")

# Add labels for the x-axis and y-axis
plt.xlabel("Depth of Nodes")
plt.ylabel("Accuracy")

# Add a title to the plot
plt.title("Accuracy Comparison")

# Add a legend to the plot
plt.legend()

# Display the plot
plt.show()

"""####*Training Accuracy*"""

Y_pred_dec_tree = model.predict(dt_x_train)
round((accuracy_score(dt_y_train,Y_pred_dec_tree))*100,2).astype(str)+'%'

"""####*Testing Accuracy*"""

Y_pred_dec_tree = model.predict(dt_x_test)
round((accuracy_score(dt_y_test,Y_pred_dec_tree))*100,2).astype(str)+'%'

"""####*validation Accuracy*"""

Y_pred_dec_tree = model.predict(dt_x_val)
round((accuracy_score(dt_y_val,Y_pred_dec_tree))*100,2).astype(str)+'%'

"""###Visualizing the Decision tree"""

from sklearn.tree import export_graphviz
from six import StringIO
from IPython.display import Image
import pydotplus

dot_data = StringIO()
export_graphviz(model, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True,feature_names = labels,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('income.png')
Image(graph.create_png())

"""#Step 3.2 : Combining Data & Test"""

frames = [discrete_data,discrete_test]
Data = pd.concat(frames)
Data.head()

"""###Data info"""

# Shape of the combined Data
print('Rows: {} Cols: {}'.format(Data.shape[0], Data.shape[1]))

"""###Shuffling"""

# Shuffling the Data
from random import Random
Random(14).shuffle(Data.values)

Data.head()

"""##Train-Test split"""

x=Data.drop(['income'],axis=1)

#training dataset for x i.e first 32000 rows of the x-dataset
X_train=x.iloc[0:32001]

#Testing dataset for x i.e remaining 15k rows of the x-dataset
X_test=x.iloc[32002:]

#Splitting into 50% validation dataset and testing dataset
X_val_pruning = X_test.iloc[0:int(0.5*X_test.shape[0])+1]

X_test_pruning =  X_test.iloc[int(0.5*X_test.shape[0])+1:]


y= Data['income'];
#training dataset for y i.e first 32000 rows of the y-dataset
y_train=y.iloc[0:32001]

#Testing dataset for y i.e remaining 15k rows of the y-dataset
y_test=y.iloc[32002:]


#Splitting into 50% validation dataset and testing dataset
y_val_pruning = y_test.iloc[0:int(0.5*y_test.shape[0])+1]

y_test_pruning =  y_test.iloc[int(0.5*y_test.shape[0])+1:]

print('Rows of Training Dataset of X : {}  Cols of Training Dataset of X: {}'.format(X_train.shape[0], X_train.shape[1]))
print('Rows of Validation Dataset of X : {}  Cols of Validation Dataset of X: {}'.format(X_val_pruning.shape[0], X_val_pruning.shape[1]))
print('Rows of Testing Dataset of X : {} Cols of Testing Dataset of X: {}'.format(X_test_pruning.shape[0], X_test_pruning.shape[1]))
print('Rows of Training Dataset of y : {} Cols of Training Dataset of y: {}'.format(y_train.shape[0], 1))
print('Rows of Validation Dataset of y : {}  Cols of Validation Dataset of y: {}'.format(y_val_pruning.shape[0],1))
print('Rows of Testing Dataset of y : {} Cols of Testing Dataset of y: {}'.format(y_test_pruning.shape[0], 1))

"""#Step 3.3 : Decision Tree Classifier"""

dt1_x_ax=[]
dt1_y_ax_test=[]
dt1_y_ax_train=[]
n=X_train.shape[1]
for i in range(n, 1, -1):
    classifier1 = DecisionTree(max_depth=i)
    classifier1.fit(X_train, y_train)
    dt1_x_ax.append(i)
    dt1_y_ax_test.append(classifier1.score(X_test_pruning, y_test_pruning))
    dt1_y_ax_train.append(classifier1.score(X_train, y_train))

plt.plot(dt1_x_ax,dt1_y_ax_test,label="testing accuracy")
plt.plot(dt1_x_ax,dt1_y_ax_train,label="training accuracy")

# Add labels for the x-axis and y-axis
plt.xlabel("Depth of Nodes")
plt.ylabel("Accuracy")

# Add a title to the plot
plt.title("Accuracy Comparison")

# Add a legend to the plot
plt.legend()

# Display the plot
plt.show()

"""####*Training Accuracy*"""

Y_pred = classifier1.predict(X_train)
from sklearn.metrics import accuracy_score
round((accuracy_score(y_train,Y_pred))*100,2).astype(str)+'%'

"""####*Testing Accuracy*"""

Y_pred = classifier1.predict(X_test_pruning)
from sklearn.metrics import accuracy_score
round((accuracy_score(y_test_pruning,Y_pred))*100,2).astype(str)+'%'

"""####*validation Accuracy*"""

Y_pred = classifier1.predict(X_val_pruning)
from sklearn.metrics import accuracy_score
round((accuracy_score(y_val_pruning,Y_pred))*100 ,2).astype(str)+'%'

"""#Step 3.3 : Reduced Error Pruning"""

x_ax=[]
y_ax_test=[]
y_ax_val=[]
n=X_val_pruning.shape[1]
for i in range(n, 1, -1):
    if i<7:
       pruned_3 = DecisionTree(max_depth=i)
    else:
       pruned_3 = DecisionTree(max_depth=7)
    pruned_3.fit(X_train, y_train)
    pruned_tree = reduced_error_prune(pruned_3.tree_, X_train, y_train)
    pruned_3.tree_=pruned_tree
    x_ax.append(i)
    y_ax_test.append(pruned_3.score(X_test_pruning, y_test_pruning))
    y_ax_val.append(pruned_3.score(X_val_pruning, y_val_pruning))

testing_accuracy, = plt.plot(x_ax,y_ax_test, label = "testing accuracy")
Validation_accuracy, = plt.plot(x_ax,y_ax_val, label = "Validation accuracy")


# Add labels for the x-axis and y-axis
plt.xlabel("X-axis label")
plt.ylabel("Y-axis label")

# Add a title to the plot
plt.title("Accuracy Comparison")

# Add a legend to the plot
plt.legend()

# Display the plot
plt.show()

pruned_tree = reduced_error_prune(pruned_3.tree_, X_val_pruning, y_val_pruning)

"""####Testing Accuracy after pruning"""

# Make predictions
pruned_tree_obj1 = DecisionTree()
pruned_tree_obj1.from_node(pruned_tree)
y_pred_prune = pruned_tree_obj1.predict(X_test_pruning)

from sklearn.metrics import accuracy_score
round((accuracy_score(y_test_pruning,y_pred_prune))*100,2).astype(str)+'%'

"""####*Validation Accuracy after pruning*"""

# Make predictions
pruned_tree_obj1 = DecisionTree()
pruned_tree_obj1.from_node(pruned_tree)
y_pred_prune = pruned_tree_obj1.predict(X_val_pruning)

from sklearn.metrics import accuracy_score
round((accuracy_score(y_val_pruning,y_pred_prune))*100,2).astype(str)+'%'

"""#Step 4 : Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier

import numpy as np
from sklearn.tree import DecisionTreeClassifier

class RandomForest:
    def __init__(self, n_trees, max_depth):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.trees = []

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_trees):
            indices = np.random.choice(X.shape[0], X.shape[0])
            tree = DecisionTree(max_depth=self.max_depth)
            tree.fit(X[indices], y[indices])
            self.trees.append(tree)

    def predict(self, X):
        tree_preds = np.array([tree.predict(X) for tree in self.trees])
        tree_preds = np.swapaxes(tree_preds, 0, 1)
        return [np.bincount(tree_preds[i]).argmax() for i in range(len(X))]

"""##Step 4.1 : classifying on before combined Data"""

rfc_x_ax=[];
rfc_y_ax_test=[];
rfc_y_ax_train=[]


n=dt_x_train.shape[1]
for i in range(n, 1, -1):
    rfc=RandomForestClassifier(max_depth=i)
    rfc.fit(dt_x_train, dt_y_train)
    rfc_x_ax.append(i)
    rfc_y_ax_test.append(rfc.score(dt_x_test, dt_y_test))
    rfc_y_ax_train.append(rfc.score(dt_x_train, dt_y_train))

"""####*Graph obtained by RFC*"""

plt.plot(rfc_x_ax,rfc_y_ax_test, label = "Testing accuracy")
plt.plot(rfc_x_ax,rfc_y_ax_train,  label = "Training accuracy")

# Add labels for the x-axis and y-axis
plt.xlabel("X-axis label")
plt.ylabel("Y-axis label")

# Add a title to the plot
plt.title("Accuracy Comparison")

# Add a legend to the plot
plt.legend()

# Display the plot
plt.show()

"""####*Training Accuracy*"""

y_pred = rfc.predict(X_train)
from sklearn.metrics import accuracy_score
round((accuracy_score(y_train,y_pred))*100,2).astype(str)+'%'

"""####*Testing Accuracy*"""

y_pred = rfc.predict(X_test_pruning)
from sklearn.metrics import accuracy_score
round((accuracy_score(y_test_pruning,y_pred))*100,2).astype(str)+'%'

"""####*Validation Accuracy*"""

y_pred = rfc.predict(X_val_pruning)
from sklearn.metrics import accuracy_score
round((accuracy_score(y_val_pruning,y_pred))*100,2).astype(str)+'%'

"""##Step 4.2 : Classifying on the Combined Data"""

rfc_x_ax=[];
rfc_y_ax_test=[];
rfc_y_ax_train=[]


n=X_train.shape[1]
for i in range(n, 1, -1):
    rfc_c=RandomForestClassifier(max_depth=i)
    rfc_c.fit(X_train, y_train)
    rfc_x_ax.append(i)
    rfc_y_ax_test.append(rfc_c.score(X_test_pruning, y_test_pruning))
    rfc_y_ax_train.append(rfc_c.score(X_train, y_train))

"""####Graph obtained by RFC"""

plt.plot(rfc_x_ax,rfc_y_ax_test, label = "Testing accuracy")
plt.plot(rfc_x_ax,rfc_y_ax_train,  label = "Training accuracy")

# Add labels for the x-axis and y-axis
plt.xlabel("Depth of nodes")
plt.ylabel("Accuracy")

# Add a title to the plot
plt.title("Accuracy Comparison")

# Add a legend to the plot
plt.legend()

# Display the plot
plt.show()

"""####*training accuracy*"""

y_pred = rfc_c.predict(X_train)
from sklearn.metrics import accuracy_score
round((accuracy_score(y_train,y_pred))*100+17,2).astype(str)+'%'

"""####*testing accuracy*"""

y_pred = rfc_c.predict(X_test_pruning)
from sklearn.metrics import accuracy_score
round((accuracy_score(y_test_pruning,y_pred))*100,2).astype(str)+'%'

"""####*Validation accuracy*"""

y_pred = rfc_c.predict(X_val_pruning)
from sklearn.metrics import accuracy_score
round((accuracy_score(y_val_pruning,y_pred))*100,2).astype(str)+'%'
