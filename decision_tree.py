# -*- coding: utf-8 -*-
"""decision Tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ln_mTJDCXCWaTXtsLlaepwqtbVWUFKP6

#**Step0: Loading the Data and Libraries**
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random as rd
data=pd.read_csv('/content/gdrive/MyDrive/ML ASSIGNMENT-1/adult.csv')
test=pd.read_csv('/content/gdrive/MyDrive/ML ASSIGNMENT-1/census-income.test.csv')

"""#**Step1: Data & Test info**"""

data.head()

test.drop(columns=test.columns[0], axis=1, inplace=True)
test.head()

# Shape of dataset
print('Rows: {} Cols: {}'.format(data.shape[0], data.shape[1]))

# Shape of Testset
print('Rows: {} Cols: {}'.format(test.shape[0], test.shape[1]))

# Features data-type
data.info()

# Features Test-type
test.info()

#Data Statistics
data.describe().T

#Test Statistics
test.describe().T

# Check for null values
round((data.isnull().sum() / data.shape[0]) * 100, 2).astype(str) + ' %'

# Check for null values
round((test.isnull().sum() / test.shape[0]) * 100, 2).astype(str) + ' %'

"""# **Step2: Data Preprocessing**

##Dataset

###*Feature Engineering Task 1*
"""

data = data.replace('?', np.nan)

# Checking null values with '?'
round((data.isnull().sum() / data.shape[0]) * 100, 2).astype(str) + ' %'

nan_cols = ['workclass', 'occupation', 'native.country']

for col in nan_cols:
    data[col].fillna(data[col].mode()[0], inplace=True)

"""###*Feature Engineering Task 2*"""

data = data.replace(0, np.nan)

# Checking null values with 0
round((data.isnull().sum() / data.shape[0]) * 100, 2).astype(str) + ' %'

zero_nan_cols = ['capital.gain','capital.loss']

for col in zero_nan_cols:
    data[col].fillna(data[col].mean(), inplace=True)

round((data.isnull().sum() / data.shape[0]) * 100, 2).astype(str) + ' %'

data.describe().T

data = data.replace('<=50K', 0)
data = data.replace('>50K', 1)
data.head(30)

"""###*Equal-Width Discretization*"""

# Define the continuous attributes that need to be discretized
continuous_attributes = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']

# Discretize the continuous attributes using appropriate techniques
num_bins = 10  # Define the number of bins for discretization

for attribute in continuous_attributes:
    # Perform discretization using the cut() function
    data[attribute+'_discrete'] = pd.cut(data[attribute], num_bins)

# Print the updated Data with discretized attributes
print(data.head())

"""###Discretized Data"""

discrete_data = data.drop(['age'],axis=1)
discrete_data = discrete_data.drop(['fnlwgt'],axis=1)
discrete_data = discrete_data.drop(['education.num'],axis=1)
discrete_data = discrete_data.drop(['capital.gain'],axis=1)
discrete_data = discrete_data.drop(['capital.loss'],axis=1)
discrete_data = discrete_data.drop(['hours.per.week'],axis=1)
discrete_data.head()

"""###Label Encoding"""

from sklearn.preprocessing import LabelEncoder

labels=['age_discrete', 'workclass', 'fnlwgt_discrete', 'education', 'education.num_discrete',
       'marital.status', 'occupation', 'relationship', 'race', 'sex',
       'capital.gain_discrete', 'capital.loss_discrete', 'hours.per.week_discrete', 'native.country']
for label in labels:
  discrete_data[label+'_n']= LabelEncoder().fit_transform(discrete_data[label])
discrete_data.head()

discrete_data=discrete_data.drop(['age_discrete', 'workclass', 'fnlwgt_discrete', 'education', 'education.num_discrete',
       'marital.status', 'occupation', 'relationship', 'race', 'sex',
       'capital.gain_discrete', 'capital.loss_discrete', 'hours.per.week_discrete', 'native.country'], axis='columns')

discrete_data.head()

"""##Test set

###Feature Engineering Task 1
"""

for col in test.columns:
    if test[col].dtype == 'object':
        test[col] = test[col].str.strip()

test.replace("?", np.nan, inplace=True)
round((test.isnull().sum() / test.shape[0]) * 100, 2).astype(str) + ' %'

nan_cols = ['Workclass','Occupation','Native country']
# test.head()

for c in nan_cols:
    test[c].fillna(test[c].mode()[0], inplace=True)

test.head()

"""###Feature Engineering Task 2"""

test = test.replace(0, np.nan)

# Checking null values with '?'
round((test.isnull().sum() / test.shape[0]) * 100, 2).astype(str) + ' %'

zero_nan_cols = ['Capital gain','Capital loss']

for c in zero_nan_cols:
    test[c].fillna(test[c].mean(), inplace=True)

test.head()

test = test.replace('<=50K.', 0)
test = test.replace('>50K.', 1)
test.head(30)

"""###*Equal-Width Discretization*"""

# Define the continuous attributes that need to be discretized
continuous_attributes = ['Age', 'Fnlwgt', 'Educatio sum', 'Capital gain', 'Capital loss', 'Hr/week']

# Discretize the continuous attributes using appropriate techniques
num_bins = 10  # Define the number of bins for discretization

for attribute in continuous_attributes:
    # Perform discretization using the cut() function
    test[attribute+'_discrete'] = pd.cut(test[attribute], num_bins)

# Print the updated Data with discretized attributes
print(test.head())

"""###Discretized Data"""

discrete_test = test.drop(['Age'],axis=1)
discrete_test = discrete_test.drop(['Fnlwgt'],axis=1)
discrete_test = discrete_test.drop(['Educatio sum'],axis=1)
discrete_test = discrete_test.drop(['Capital gain'],axis=1)
discrete_test = discrete_test.drop(['Capital loss'],axis=1)
discrete_test = discrete_test.drop(['Hr/week'],axis=1)
discrete_test.shape[0]
discrete_test.head()

# Rename column 'A' to 'X'
discrete_test = discrete_test.rename(columns={'Occupation':'occupation','Maritial Status':'marital.status','Relationship':'relationship','Workclass':'workclass','Education':'education','Race':'race','Sex':'sex','Native country':'native.country','Class':'income','Hr/week_discrete':'hours.per.week_discrete','Age_discrete': 'age_discrete','Fnlwgt_discrete':'fnlwgt_discrete','Educatio sum_discrete':'education.num_discrete','Capital gain_discrete':'capital.gain_discrete','Capital loss_discrete':'capital.loss_discrete'})

"""###Label Encoding"""

from sklearn.preprocessing import LabelEncoder

labels=['age_discrete', 'workclass', 'fnlwgt_discrete', 'education', 'education.num_discrete',
       'marital.status', 'occupation', 'relationship', 'race', 'sex',
       'capital.gain_discrete', 'capital.loss_discrete', 'hours.per.week_discrete', 'native.country']
for label in labels:
  discrete_test[label+'_n']= LabelEncoder().fit_transform(discrete_test[label])
discrete_test.head()

discrete_test=discrete_test.drop(['age_discrete', 'workclass', 'fnlwgt_discrete', 'education', 'education.num_discrete',
       'marital.status', 'occupation', 'relationship', 'race', 'sex',
       'capital.gain_discrete', 'capital.loss_discrete', 'hours.per.week_discrete', 'native.country'], axis='columns')

discrete_test.head()

"""# Step 3.1 : Decision Tree Classifier

##Node class
"""

class Node():
  def __init__(self, feature_index=None, threshold=None, left= None, right = None, info_gain= None, value= None):
      ''' calling the constructor '''

      # Parameters for decision Node
      self.feature_index=feature_index;
      self.threshold=threshold;
      self.left=left;
      self.right=right;
      self.info_gain=info_gain;

      # Parameters for External(leaf) Node
      self.value = value;

"""##Tree class"""

# class DTClassifier():
#   def __init__(self, minimum_samples_split=2, maximum_depth=2):
#     ''' calling the constructor '''

#     #init the main root of the tree
#     self.root = None

#     #Terminating Conditions
#     self.minimum_samples_split = minimum_samples_split
#     self.maximum_depth = maximum_depth



#   def split(self, dataset, feature_index, threshold):
#         ''' splitting the data '''

#         dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])
#         dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])
#         return dataset_left, dataset_right

#   def entropy(self, y):
#         ''' computing entropy '''

#         class_labels = np.unique(y)
#         entropy = 0
#         for i in class_labels:
#             prob = len(y[y == i]) / len(y)
#             entropy += -prob * np.log2(prob)
#         return entropy

#   def gini_index(self, y):
#         ''' computing gini index '''

#         class_labels = np.unique(y)
#         gini = 0
#         for i in class_labels:
#             prob = len(y[y == i]) / len(y)
#             gini += prob*prob
#         return 1 - gini

#   def calculate_leaf_value(self, Y):
#         '''computing leaf node '''

#         Y = list(Y)
#         return max(Y, key=Y.count)


#   def information_gain(self, parent, left_child, right_child, mode="entropy"):
#         ''' calculating information gain '''

#         left_weight_average = len(left_child) / len(parent)
#         right_weight_average = len(right_child) / len(parent)
#         if mode=="gini":
#             gain = self.gini_index(parent) - (left_weight_average*self.gini_index(left_child) + right_weight_average*self.gini_index(right_child))
#         else:
#             gain = self.entropy(parent) - (left_weight_average*self.entropy(left_child) + right_weight_average*self.entropy(right_child))
#         return gain

#   def print_tree(self, tree=None, indent=" "):
#         ''' print the tree '''

#         if not tree:
#             tree = self.root

#         if tree.value is not None:
#             print(tree.value)

#         else:
#             print("X_"+str(tree.feature_index), "<=", tree.threshold, "?", tree.info_gain)
#             print("%sleft:" % (indent), end="")
#             self.print_tree(tree.left, indent + indent)
#             print("%sright:" % (indent), end="")
#             self.print_tree(tree.right, indent + indent)


#   def get_best_split(self, dataset, number_of_samples, number_of_features):
#         ''' finding the best split '''

#         # dictionary to store the best split
#         best_split = {}
#         #initializing maximum gain value as min value of float
#         max_info_gain = -float("inf")

#         # loop over all the features
#         for feature_index in range(number_of_features):

#             #first row - last row of column-feature_index
#             feature_values = dataset[:, feature_index]

#             #extracting only the unique values
#             possible_thresholds = np.unique(feature_values)

#             # loop over all the feature values present in the data
#             for threshold in possible_thresholds:

#                 # get current split
#                 dataset_left, dataset_right = self.split(dataset, feature_index, threshold)

#                 # check if childs are not null
#                 if len(dataset_left)>0 and len(dataset_right)>0:
#                     y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]

#                     # compute information gain
#                     curr_info_gain = self.information_gain(y, left_y, right_y, "gini")

#                     # update the best split if needed
#                     if curr_info_gain>max_info_gain:
#                         best_split["feature_index"] = feature_index
#                         best_split["threshold"] = threshold
#                         best_split["dataset_left"] = dataset_left
#                         best_split["dataset_right"] = dataset_right
#                         best_split["info_gain"] = curr_info_gain
#                         max_info_gain = curr_info_gain

#         # return best split
#         return best_split

#   def build_tree(self, dataset, current_depth=0):

#     #starts from the end, towards the first, taking each element in reverse order
#     X = dataset[:,:-1]
#     Y = dataset[:,-1]
#     number_of_samples = np.shape(X)
#     number_of_features = np.shape(X)

#     # split until terminating conditions are met
#     if number_of_samples>=self.minimum_samples_split and current_depth<=self.maximum_depth:
#         # find the best split
#         best_split = self.get_best_split(dataset, number_of_samples, number_of_features)
#         # check if information gain is positive
#         if best_split["info_gain"]>0:

#             # recursion left
#             left_subtree = self.build_tree(best_split["dataset_left"], current_depth+1)
#             # recursion right
#             right_subtree = self.build_tree(best_split["dataset_right"], current_depth+1)

#             # return decision node
#             return Node(best_split["feature_index"], best_split["threshold"],
#                     left_subtree, right_subtree, best_split["info_gain"])

#     # compute leaf Node
#     leafVal = self.calculate_leafVal(Y)
#     # return leaf node
#     return Node(value=leaf_value)


#   def fit(self, X):
#         ''' function to train the tree '''


#         self.root = self.build_tree(X)


#   def predict(self, X):
#         ''' function to predict new dataset '''

#         preditions = [self.make_prediction(x, self.root) for x in X]
#         return preditions


#   def make_prediction(self, x, tree):
#         ''' function to predict a single data point '''

#         if tree.value!=None: return tree.value
#         feature_val = x[tree.feature_index]
#         if feature_val<=tree.threshold:
#             return self.make_prediction(x, tree.left)
#         else:
#             return self.make_prediction(x, tree.right)



class DecisionTreeClassifier:
    def __init__(self, max_depth=None, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split

    def fit(self, X, y):
        self.n_classes = len(np.unique(y))
        self.n_features = X.shape[1]
        self.tree = self._grow_tree(X, y)

    def _calculate_gini(self, y):
        _, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        gini = 1 - np.sum(probabilities ** 2)
        return gini

    def _best_split(self, X, y):
        best_gini = float('inf')
        best_feature = None
        best_threshold = None

        for feature in range(self.n_features):
            if isinstance(X, np.ndarray):
                thresholds = np.unique(X[:, feature])
            else:  # Assume X is a pandas DataFrame or Series
                thresholds = X.iloc[:, feature].unique()

            for threshold in thresholds:
                if isinstance(X, np.ndarray):
                    left_indices = X[:, feature] <= threshold
                    right_indices = X[:, feature] > threshold
                else:  # Assume X is a pandas DataFrame or Series
                    left_indices = X.iloc[:, feature] <= threshold
                    right_indices = X.iloc[:, feature] > threshold

                left_gini = self._calculate_gini(y[left_indices])
                right_gini = self._calculate_gini(y[right_indices])

                gini = (len(y[left_indices]) / len(y)) * left_gini + (len(y[right_indices]) / len(y)) * right_gini

                if gini < best_gini:
                    best_gini = gini
                    best_feature = feature
                    best_threshold = threshold

        return best_feature, best_threshold

    def _grow_tree(self, X, y, depth=0):
        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes)]
        predicted_class = np.argmax(num_samples_per_class)

        node = {'predicted_class': predicted_class}

        if depth < self.max_depth and len(X) >= self.min_samples_split:
            feature, threshold = self._best_split(X, y)
            if feature is not None:
                node['feature'] = feature
                node['threshold'] = threshold
                node['children'] = {}

                if isinstance(X, np.ndarray):
                    unique_values = np.unique(X[:, feature])
                else:  # Assume X is a pandas DataFrame or Series
                    unique_values = X.iloc[:, feature].unique()

                for value in unique_values:
                    if isinstance(X, np.ndarray):
                        indices = X[:, feature] == value
                    else:  # Assume X is a pandas DataFrame or Series
                        indices = X.iloc[:, feature] == value

                    X_child = X[indices]
                    y_child = y[indices]
                    node['children'][value] = self._grow_tree(X_child, y_child, depth+1)

        return node

    def _predict_single_sample(self, x, node):
        if 'predicted_class' in node:
            return node['predicted_class']

        feature = node['feature']
        value = x[feature]

        if value in node['children']:
            child = node['children'][value]
            return self._predict_single_sample(x, child)

        return node['predicted_class']

    def predict(self, X):
        if isinstance(X, pd.DataFrame):
            X = X.values

        y_pred = np.zeros(len(X))
        for i, x in enumerate(X):
            y_pred[i] = self._predict_single_sample(x, self.tree)
        return y_pred

    def split(self, dataset, feature_index, threshold):
        ''' Splitting the data '''
        dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold])
        dataset_right = np.array([row for row in dataset if row[feature_index] > threshold])
        return dataset_left, dataset_right

"""##Fit the Model"""

dt_y_train=discrete_data.income
dt_x_train=discrete_data.drop(['income'],axis=1);
dt_y_test=discrete_test.income
dt_x_test=discrete_test.drop(['income'],axis=1);

from sklearn.tree import DecisionTreeClassifier
n=dt_x_train.shape[0]
dt_x_ax=[]
dt_y_ax_test=[]
dt_y_ax_train=[]

for i in range(n, 1, -14):
    model = DecisionTreeClassifier(max_leaf_nodes=i)
    model.fit(dt_x_train, dt_y_train)
    dt_x_ax.append(i)
    dt_y_ax_test.append(model.score(dt_x_test, dt_y_test))
    dt_y_ax_train.append(model.score(dt_x_train, dt_y_train))
plt.plot(dt_x_ax,dt_y_ax_test)
plt.plot(dt_x_ax,dt_y_ax_train)

"""###Visualizing the Decision tree"""

from sklearn.tree import export_graphviz
from six import StringIO
from IPython.display import Image
import pydotplus

dot_data = StringIO()
export_graphviz(model, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True,feature_names = labels,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
graph.write_png('income.png')
Image(graph.create_png())

"""####*Training Acuuracy*"""

round((model.score(dt_x_train,dt_y_train)) * 100, 2).astype(str) + ' %'

"""####*Testing Accuracy*"""

round((model.score(dt_x_test,dt_y_test)) * 100, 2).astype(str) + ' %'

"""#Step 3.2 : Combining Data & Test"""

frames = [discrete_data,discrete_test]
Data = pd.concat(frames)
Data.head()

"""###Data info"""

# Shape of the combined Data
print('Rows: {} Cols: {}'.format(Data.shape[0], Data.shape[1]))

"""###Shuffling"""

# Shuffling the Data
from random import Random
Random(14).shuffle(Data.values)

Data.head()

"""##Train-Test split"""

x=Data.drop(['income'],axis=1)

#training dataset for x i.e first 32000 rows of the x-dataset
X_train=x.iloc[0:32001]

#Testing dataset for x i.e remaining 15k rows of the x-dataset
X_test=x.iloc[32002:]

#Splitting into 50% validation dataset and testing dataset
X_val_pruning = X_test.iloc[0:int(0.5*X_test.shape[0])+1]

X_test_pruning =  X_test.iloc[int(0.5*X_test.shape[0])+1:]


y= Data['income'];
#training dataset for y i.e first 32000 rows of the y-dataset
y_train=y.iloc[0:32001]

#Testing dataset for y i.e remaining 15k rows of the y-dataset
y_test=y.iloc[32002:]


#Splitting into 50% validation dataset and testing dataset
y_val_pruning = y_test.iloc[0:int(0.5*y_test.shape[0])+1]

y_test_pruning =  y_test.iloc[int(0.5*y_test.shape[0])+1:]

print('Rows of Training Dataset of X : {}  Cols of Training Dataset of X: {}'.format(X_train.shape[0], X_train.shape[1]))
print('Rows of Validation Dataset of X : {}  Cols of Validation Dataset of X: {}'.format(X_val_pruning.shape[0], X_val_pruning.shape[1]))
print('Rows of Testing Dataset of X : {} Cols of Testing Dataset of X: {}'.format(X_test_pruning.shape[0], X_test_pruning.shape[1]))
print('Rows of Training Dataset of y : {} Cols of Training Dataset of y: {}'.format(y_train.shape[0], 1))
print('Rows of Validation Dataset of y : {}  Cols of Validation Dataset of y: {}'.format(y_val_pruning.shape[0],1))
print('Rows of Testing Dataset of y : {} Cols of Testing Dataset of y: {}'.format(y_test_pruning.shape[0], 1))

"""#Step 3.3 : Reduced Error Pruning"""

from sklearn.preprocessing import scale
from copy import deepcopy